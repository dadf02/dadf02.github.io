{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "895c7a1c",
   "metadata": {},
   "source": [
    "# Cheat sheet for ADA coding part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e039e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn', Mutes warnings when copying a slice from a DataFrame.\n",
    "import scipy as sp\n",
    "\n",
    "#Stats\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import patsy\n",
    "\n",
    "# ML\n",
    "from sklearn import (preprocessing,model_selection,metrics,linear_model,ensemble,decomposition)\n",
    "import joblib\n",
    "\n",
    "#GRaphs\n",
    "import networkx as nx\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#Utilities\n",
    "import requests\n",
    "\n",
    "# Jupyter / IPython\n",
    "import ipykernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9bba8a",
   "metadata": {},
   "source": [
    "## 1) Handling data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba77a10",
   "metadata": {},
   "source": [
    "### 1.1 Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34fe11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = pd.Series([1,2,3,4])\n",
    "print(counts.values)\n",
    "print(counts.index)\n",
    "\n",
    "counts = pd.Series([1,2,3,4], index =['uno','dos','tres','cuatro'])\n",
    "print(counts.values)\n",
    "print(counts.index)\n",
    "\n",
    "print(counts[\"uno\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d25f287",
   "metadata": {},
   "source": [
    "Endswith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1471a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts[[name.endswith('o') for name in counts.index]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73eb712a",
   "metadata": {},
   "source": [
    "We can index the series by their position. And we can apply operations `bacteria.apply(np.log)` and filtering `bacteria[bacteria>1000]`. We can addup two series (the indexes will be used to match) and see if something is null: `bacteria2.isnull()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a7b4c1",
   "metadata": {},
   "source": [
    "### 1.2) Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a2674d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'value':[632, 1638, 569, 115, 433, 1130, 754, 555],\n",
    "                     'patient':[1, 1, 1, 1, 2, 2, 2, 2],\n",
    "                     'phylum':['Firmicutes', 'Proteobacteria', 'Actinobacteria', \n",
    "    'Bacteroidetes', 'Firmicutes', 'Proteobacteria', 'Actinobacteria', 'Bacteroidetes']})\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f53372",
   "metadata": {},
   "source": [
    "#### Basics operations: \n",
    "\n",
    "- `data[['phylum','phylum','patient']]`: change columns order. \n",
    "- `df.rename(columns={\"phylum\": \"a\", \"phylum\": \"c\"})`: change column names. \n",
    "- `data.dtypes`: return types of values of each column. \n",
    "- `data['patient']`, `data.patient`, `data[['value']]`, : access a column. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5368ac0",
   "metadata": {},
   "source": [
    "#### Loc and iloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ce51d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"name\": [\"Ana\", \"Ben\", \"Carl\", \"Dina\"],\"age\": [23, 25, 22, 30],\"city\": [\"Paris\", \"Berlin\", \"Rome\", \"Madrid\"]}, index=[\"a\", \"b\", \"c\", \"d\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206e2ff2",
   "metadata": {},
   "source": [
    "- `df.loc[\"b\"]`: returns the row b. \n",
    "- `df.loc[\"a\":\"c\", [\"name\", \"age\"]]`: returns the columns name and age of the rows a and c. \n",
    "- `df.loc[\"c\", \"city\"]`: specific value at row c and column city. \n",
    "- `df.iloc[1]`: second row. \n",
    "- `df.iloc[[0, 2]]`: first and third row. \n",
    "- `df.iloc[0:3, 0:2]`: select rows and columns. \n",
    "- `df.iloc[2, 2]`: single values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca3d146",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GET ROWS WHERE CONDITION HOLDS\n",
    "df.loc[df[\"city\"] == \"Berlin\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cbce88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GET SPECIFIC COLUMNS AFTER FILTERING\n",
    "df.loc[df[\"age\"] > 23, [\"name\", \"city\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85bb4b3",
   "metadata": {},
   "source": [
    "Remember: if we create a variable that is a series of a column of a dataframe -> this references to the original data and all modifs here will take effect on the original one. Better to do a copy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f2cb18",
   "metadata": {},
   "source": [
    "#### Filtering with apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549260f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame([{'patient': 1, 'phylum': 'Firmicutes', 'value': 632},{'patient': 1, 'phylum': 'Proteobacteria', 'value': 1638},{'patient': 1, 'phylum': 'Actinobacteria', 'value': 569},{'patient': 1, 'phylum': 'Bacteroidetes', 'value': 115},{'patient': 2, 'phylum': 'Firmicutes', 'value': 433},{'patient': 2, 'phylum': 'Proteobacteria', 'value': 1130},{'patient': 2, 'phylum': 'Actinobacteria', 'value': 754},{'patient': 2, 'phylum': 'Bacteroidetes', 'value': 555}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0329dc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['phylum'].apply(lambda x: x.endswith('bacteria')) & data['value'].apply(lambda x: x>1000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f3c4f2",
   "metadata": {},
   "source": [
    "#### Drop a column or row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1faa7804",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nophylum = data.drop('phylum', axis=1) #DROPS phylum colum\n",
    "data_no0 = data.drop(0) #DROPS first row."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b509fb",
   "metadata": {},
   "source": [
    "### Importing files\n",
    "\n",
    "- `pd.read_csv(\"Data/microbiome.csv\", sep=',')`: standard stuff. \n",
    "- `pd.read_csv(\"Data/microbiome.csv\", sep=',', decimal = ',')`: if a decimal column is using a ,. \n",
    "- `pd.read_csv(\"Data/microbiome.csv\", header=None)`: have no header. \n",
    "- `pd.read_csv(\"Data/microbiome.csv\", index_col=['Patient'])`: specific one column as the index. (`df.set_index('month')`)\n",
    "- `pd.read_csv(\"Data/microbiome.csv\", skiprows=[3,4,6]).head()`: skip some rows. \n",
    "- `pd.read_csv(\"Data/microbiome.csv\", nrows=4)`: import only some rows. \n",
    "- `pd.read_csv(\"Data/microbiome_missing.csv\", na_values=['?', -99999])`: specifiy nan_values to be recognized. \n",
    "- `pd.isnull(df)`: return a boolean at each position to see if it is nan or not. \n",
    "- `pd.read_excel('Data/microbiome_MID2.xls', sheet_name='Sheet 1', header=None)`: import excel file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b395f25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Read a json\n",
    "df = pd.read_json(\"./data/exam1.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f627159b",
   "metadata": {},
   "source": [
    "#### Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fdc209",
   "metadata": {},
   "source": [
    "- `baseball.index.is_unique`: verify index of a dataframe (baseball) is unique, returns boleean. \n",
    "- `player_id = baseball.player + baseball.team + baseball.year.astype(str)`: creates column combining strings. \n",
    "- `baseball_newind['womacto01CHN2006':'gonzalu01ARI2006']`: select some rows indexing with the index as in numpy arrays. \n",
    "- `baseball_newind.query('ab > 500') = baseball_newind.loc[baseball_newind[\"ab\"] > 500]`: can use query for conditions too. \n",
    "- `df.query(\"ab > 500 and hr > 20\")`: several conditions with query. \n",
    "- `min_ab=500;baseball_newind.query(\"ab > @min_ab\")`: compares with a global variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36c5382",
   "metadata": {},
   "source": [
    "#### isin\n",
    "\n",
    "- `data['phylum'].isin(['Firmicutes', 'Bacteroidetes'])`: recover the data in which the columns match with it\n",
    "- `df.query(\"team in ['BOS', 'NYY', 'LAD']\") = df[df[\"team\"].isin([\"BOS\", \"NYY\", \"LAD\"])]`: how to do the same with query. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a61bd0",
   "metadata": {},
   "source": [
    "#### Operations\n",
    "- `hr2006 = baseball.loc[baseball.year==2006, 'hr']`: select a year and keep only one column. \n",
    "- `hr2007.add(hr2006, fill_value=0)`: fill nans when performing sums of two series with indexes that may differ btw them. \n",
    "- `(baseball.hr - baseball.hr.max())`: subtract a column from a specific value.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b922a0a9",
   "metadata": {},
   "source": [
    "#### SORTING\n",
    "\n",
    "- `baseball_newind.sort_index()`: sort using the index.\n",
    "- `baseball_newind.sort_index(ascending=False)`: sort using index in reverse order. \n",
    "- `df.sort_values(by=\"age\", ascending=False)`: ORDENAR DE MAYOR A MENOR.  \n",
    "- `baseball[['player','sb','cs']].sort_values(ascending=[False,True], by=['sb', 'cs'])`: sort using several columns.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44a5e10",
   "metadata": {},
   "source": [
    "#### Calculate an arithmetric formula given other values in a dataframe: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ecacf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['obp'] = df.apply(lambda p: (p.h+p.bb+p.hbp)/(p.ab+p.bb+p.hbp+p.sf) if (p.ab+p.bb+p.hbp+p.sf) != 0.0 else 0.0, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8da4ff",
   "metadata": {},
   "source": [
    "### Missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f26d2c",
   "metadata": {},
   "source": [
    "- `foo.isnull()`: return booleans in positions where there's a nan. \n",
    "- `data.dropna()`: drop row with atleast one nan (change axis to 1 for column, and how to all for all rows/columns). \n",
    "- `bacteria2.fillna(0)`: fill nans with zeros. \n",
    "- `data.fillna({'year': 2013, 'treatment':2})`: fill nans for specific columns (use inplace = TRUE to change original datset). \n",
    "- `bacteria2.fillna(method='bfill')`: fill with interpolation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c256947",
   "metadata": {},
   "source": [
    "#### Descriptions\n",
    "\n",
    "- `pd.sum()`.\n",
    "- `pd.mean()`. \n",
    "- `bacteria2.mean(skipna=False)`: mean ignoring nans. \n",
    "- `baseball.describe()`: general data description of the database. \n",
    "- `baseball.hr.cov(baseball.X2b)`.\n",
    "- `baseball.hr.corr(baseball.X2b)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da46d70f",
   "metadata": {},
   "source": [
    "#### Export"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f108cc19",
   "metadata": {},
   "source": [
    "- `mb.to_csv(\"mb.csv\")`\n",
    "- `baseball.to_pickle(\"baseball_pickle\")`: binary format.\n",
    "- `pd.read_pickle(\"baseball_pickle\").head()`: read binary format. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5c661f",
   "metadata": {},
   "source": [
    "### Date and time formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b875290b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, date, time\n",
    "now = datetime.now()\n",
    "now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4634ad9",
   "metadata": {},
   "source": [
    "- `date(1970, 9, 3)`: create object with date information. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b50b463",
   "metadata": {},
   "source": [
    "### MERGEEEEEEEE !!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bec35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "people = pd.DataFrame({\"id\": [1, 2, 3],\"name\": [\"Ana\", \"Ben\", \"Cara\"]})\n",
    "scores = pd.DataFrame({\"id\": [2, 3, 4],\"score\": [88, 92, 75]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e15bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef12a0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa8e6c9",
   "metadata": {},
   "source": [
    "#### Four types of merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13befc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(people, scores, on=\"id\", how=\"inner\") #KEEP ROWS THAT MATCH ONLY BOTH DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4defdc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(people, scores, on=\"id\", how=\"left\") #KEEP ROWS THAT ARE ON THE LEFT ONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771d372a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(people, scores, on=\"id\", how=\"right\") #ON THE RIGHT ONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43f94dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(people, scores, on=\"id\", how=\"outer\") #KEEP EVERYTHING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4d6b37",
   "metadata": {},
   "source": [
    "#### Different column names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305ab15c",
   "metadata": {},
   "source": [
    "Use \"on\" if both datasets have a column with the same name. Otherwise, use left and right on: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9323fa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores2 = pd.DataFrame({\"person_id\": [2, 3, 4],\"score\": [88, 92, 75]})\n",
    "pd.merge(people,scores2,left_on=\"id\",right_on=\"person_id\",how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c5d004",
   "metadata": {},
   "source": [
    "#### Merge on index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ada6864",
   "metadata": {},
   "outputs": [],
   "source": [
    "people_idx = people.set_index(\"id\")\n",
    "scores_idx = scores.set_index(\"id\")\n",
    "\n",
    "pd.merge(people_idx,scores_idx,left_index=True,right_index=True,how=\"inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfb76c2",
   "metadata": {},
   "source": [
    "#### Merge column and index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2247d1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(people,scores_idx,left_on=\"id\",right_index=True,how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0813858f",
   "metadata": {},
   "source": [
    "IT IS IMPERATIVE FOR THE COLUMNS TO MATCH OTHERWISE IT WILL NOT WORK.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a9e483",
   "metadata": {},
   "source": [
    "#### Suffixes when there is repeated columns names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f021e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({\"id\": [1, 2],\"value\": [10, 20]})\n",
    "df2 = pd.DataFrame({\"id\": [2, 3],\"value\": [200, 300]})\n",
    "pd.merge(df1, df2, on=\"id\", how=\"outer\", suffixes=(\"_left\", \"_right\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0faa0a",
   "metadata": {},
   "source": [
    "### Concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e735e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({\"id\": [1, 2],\"name\": [\"Ana\", \"Ben\"]})\n",
    "df2 = pd.DataFrame({\"id\": [3, 4],\"name\": [\"Cara\", \"Dan\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07886e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df1, df2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f97f630",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df1, df2], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01709fdc",
   "metadata": {},
   "source": [
    "Combining data with not same columns: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8620d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.DataFrame({\"id\": [5],\"age\": [30]})\n",
    "pd.concat([df1, df3], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21afd9e",
   "metadata": {},
   "source": [
    "Stack columns: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3645065",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfA = pd.DataFrame({\"name\": [\"Ana\", \"Ben\", \"Cara\"]}, index=[0, 1, 2])\n",
    "dfB = pd.DataFrame({\"score\": [88, 92]}, index=[1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5d6f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([dfA, dfB], axis=1) #DONE BY THE INDEXXXXXXXX!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9d3cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([dfA, dfB], axis=1, join=\"inner\") #Same but keeping only common things"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8910d1",
   "metadata": {},
   "source": [
    "### Reshaping DataFrame objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76e220d",
   "metadata": {},
   "source": [
    "- `stacked = cdystonia.stack()`: rotates so that all columns are presented in rows. \n",
    "- `stacked.unstack().head()?`: pivots rows back to columns. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050157d5",
   "metadata": {},
   "source": [
    "### Pivoting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f18975e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdystonia = pd.DataFrame({\"patient\": [1,1,1, 2,2,2, 3,3,3],\"site\":    [\"A\",\"A\",\"A\",\"B\",\"B\",\"B\",\"A\",\"A\",\"A\"],\"id\":      [101,101,101, 102,102,102, 103,103,103],\"treat\":   [\"drug\",\"drug\",\"drug\", \"placebo\",\"placebo\",\"placebo\", \"drug\",\"drug\",\"drug\"],\"age\":     [60,60,60, 55,55,55, 47,47,47],\"sex\":     [\"F\",\"F\",\"F\", \"M\",\"M\",\"M\", \"F\",\"F\",\"F\"],\"obs\":     [1,2,3, 1,2,3, 1,2,3],          \"week\":    [0,4,8, 0,4,8, 0,4,8],          \"twstrs\":  [35,30,28, 40,41,39, 25,20,18]  })\n",
    "cdystonia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13946f78",
   "metadata": {},
   "source": [
    "We need three values for the pivot: \n",
    "\n",
    "- index='patient' → each row becomes one patient\n",
    "- columns='obs' → each column becomes an obs number (1, 2, 3, …)\n",
    "- values='twstrs' → what you fill inside the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77c284a",
   "metadata": {},
   "outputs": [],
   "source": [
    "twstrs_wide = cdystonia.pivot(index='patient', columns='obs', values='twstrs').head()\n",
    "twstrs_wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab1c6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdystonia[['patient','site','id','treat','age','sex']].drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedadfa7",
   "metadata": {},
   "source": [
    "Here we keep only one row for each patient. So only one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37598a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdystonia_wide = (cdystonia[['patient','site','id','treat','age','sex']].drop_duplicates()\n",
    "      .merge(twstrs_wide, right_index=True, left_on='patient', how='inner'))\n",
    "cdystonia_wide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53466ea9",
   "metadata": {},
   "source": [
    "Here, we have taken the dropped_duplicates dataset and merged it with the pivot table we created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68757ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.melt(cdystonia_wide,id_vars=['patient','site','id','treat','age','sex'],var_name='obs',value_name='twsters')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4a729b",
   "metadata": {},
   "source": [
    "From this, we have that we have remade the table long from a wide format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d5d7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdystonia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f3bd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdystonia.pivot_table(index=['site', 'treat'],columns='week',values='twstrs',aggfunc=max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f327da",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(cdystonia.sex, cdystonia.site)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8664bae3",
   "metadata": {},
   "source": [
    "### Data Transformation \n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b65da6",
   "metadata": {},
   "source": [
    "### Categorical data\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ba0399",
   "metadata": {},
   "source": [
    "### Data aggregation and GroupBy operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70abd227",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_proportion_positive = df.groupby([\"YEA\"]).VOT.apply(lambda x: np.mean(x > 0)).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a23aac",
   "metadata": {},
   "source": [
    "### Flat a list of lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459a0f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"token_flat\"] =  df[\"tokens\"].apply(lambda xss: [x for xs in xss for x in xs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fd3e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = (sorted(set(list_of_words_flat))) #### TO get sorted list of characters in a list -> it's alphabetical and it's a list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a870eb",
   "metadata": {},
   "source": [
    "## 2) Data visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0815ae2d",
   "metadata": {},
   "source": [
    "### General plotting - histogram:\n",
    "\n",
    "- `df[column].hist(bins=whathevernumberyouwant)`: histogram of a single variable of a dataset.\n",
    "```\n",
    "plt.xlabel('Worldwide gross revenue')\n",
    "plt.ylabel('Number of movies')\n",
    "plt.title('Gross revenue, histogram');\n",
    "``` \n",
    "- `plt.hist(movies['worldwide_gross'].values, bins = 100)`: exact same thing but with matplotlib\n",
    "- `segments.seg_length.apply(np.log).hist(bins=500)`: APPPLY A TRANSFORMATION AND THEN PLOT THE STUFF.\n",
    "\n",
    "### SNS (prettier):\n",
    "\n",
    "```\n",
    "ax = sns.histplot(treated['re78'], kde=True, stat='density', color='blue', label='treated');\n",
    "ax = sns.histplot(control['re78'], kde=True, stat='density', color='orange', label='control')\n",
    "ax.set(title='Income distribution comparison in 1978, after matching',xlabel='Income 1978', ylabel='Income density')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6d447a",
   "metadata": {},
   "source": [
    "### Boxplot:\n",
    "```\n",
    "plt.boxplot(movies['worldwide_gross'])\n",
    "plt.xticks([])\n",
    "plt.title('Worldwide gross revenue');\n",
    "```\n",
    "\n",
    "\n",
    "### BOXPLOT BETTER: \n",
    "\n",
    "``` df.boxplot(by=\"treat\", column=\"age\", grid=True)```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47c68dd",
   "metadata": {},
   "source": [
    "### Scatter plots "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4cc222",
   "metadata": {},
   "source": [
    "- `df.plot.scatter(x='hr', y='X2b')`: scatter plot using two columns of a dataset. $\\newline$\n",
    "Alternative:\n",
    "``` \n",
    "plt.scatter(movies['worldwide_gross'], movies['imdb_rating'], s = 2)\n",
    "plt.xlabel('Worldwide gross revenue')\n",
    "plt.ylabel('IMDB rating')\n",
    "```\n",
    "\n",
    "We use the \"s\" parameter to adjust the size of points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cd5bed",
   "metadata": {},
   "source": [
    "### LM plot: linear regression:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c68d08",
   "metadata": {},
   "source": [
    "USE HUE TO SEPARATE POINTS !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2d23f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x='SelfEmployed',y='IncomePerCap', data=df, hue = 'State')\n",
    "plt.xlabel(\"Percentage of Self Employed people [%]\")\n",
    "plt.ylabel(\"Income per Capita [$]\")\n",
    "plt.ylim([10000,50000])\n",
    "plt.xlim([0,22])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f301c3c3",
   "metadata": {},
   "source": [
    "### Double plots in seaborn: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601b4eb5",
   "metadata": {},
   "source": [
    "- `sns.jointplot(x=movies['worldwide_gross'], y=movies['imdb_rating'], kind=\"hex\")`: scatter plot + bar plots.\n",
    "- `sns.jointplot(x=movies['worldwide_gross'], y=movies['imdb_rating'], kind=\"kde\")`: scatter plot + kde curves.\n",
    "- `sns.jointplot(data = movies, x = 'worldwide_gross', y = 'imdb_rating', kind=\"reg\")`: scatter plot + reg lines. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67baa781",
   "metadata": {},
   "source": [
    "### Visualize columns of a dataset using boxplots, violinplots and barplots:\n",
    "\n",
    "- Barplots: `sns.barplot(x=\"Main_Genre\", y=\"worldwide_gross\", data=movies.loc[movies['Main_Genre'].isin(['Thriller','Comedy' 'Fantasy','Sci-Fi','Romance'])])`\n",
    "- Boxplots: `sns.boxplot(x=\"Main_Genre\", y=\"worldwide_gross\", data=movies.loc[movies['Main_Genre'].isin(['Thriller','Comedy','Fantasy','Sci-Fi','Romance'])])`\n",
    "- Violinplots: `sns.violinplot(x=\"Main_Genre\", y=\"worldwide_gross\", data=movies.loc[movies['Main_Genre'].isin(['Thriller','Comedy','Fantasy','Sci-Fi','Romance'])])`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340b7dcb",
   "metadata": {},
   "source": [
    "#### PLots in seaborn with error bars: \n",
    "-`ax = sns.barplot(x=\"State\", y=\"IncomePerCap\", data=df.loc[df['State'].isin(['New York','California'])])\n",
    "plt.ylim([25000,32000])`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a946d74",
   "metadata": {},
   "source": [
    "### MAKE A 4x4 plot of histograms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6afb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_by_genre = df.groupby('Main_Genre').apply(lambda x: pd.Series({'length': x['length'].values}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c070521",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4,4,figsize= (8,6), sharey = True, sharex = True)\n",
    "for i in range(16):\n",
    "    sbplt = ax[i%4, math.floor(i/4)]\n",
    "    sbplt.hist(stats_by_genre.iloc[i].values,range = [0,200],bins = 20)\n",
    "    sbplt.set_title(stats_by_genre.index[i])    \n",
    "fig.tight_layout()\n",
    "fig.text(0.4,0, \"Movie length in minutes\")\n",
    "fig.text(0,0.6, \"Number of movies\", rotation = 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84484e3c",
   "metadata": {},
   "source": [
    "### Make heatmaps: \n",
    "\n",
    "#Two variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0103fc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code to make the first heatmap here\n",
    "heat1= pd.crosstab(df[\"Main_Genre\"], df[\"studio\"])\n",
    "sns.heatmap(heat1,annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69b3a83",
   "metadata": {},
   "source": [
    "#Three variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48598bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code to make the second heatmap here\n",
    "heat2= pd.crosstab(df[\"Main_Genre\"], df[\"Genre_2\"], values=df[\"worldwide_gross\"], aggfunc='mean')\n",
    "sns.heatmap(heat2,annot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63690adb",
   "metadata": {},
   "source": [
    "### Plots with error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03baab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_plot_revenue = movies.groupby(\"year\")[\"worldwide_gross\"].agg([('mean', 'mean'), ('std', 'std')]) \n",
    "\n",
    "#Error bars:\n",
    "plt.plot(data_plot_revenue.index.values,data_plot_revenue[\"mean\"].values)\n",
    "plt.errorbar(data_plot_revenue.index.values,data_plot_revenue[\"mean\"].values,yerr = data_plot_revenue[\"std\"].values,fmt ='o')\n",
    "\n",
    "#Filled plot here\n",
    "plt.plot(data_plot_revenue.index.values,data_plot_revenue[\"mean\"].values, \"k-\")\n",
    "plt.fill_between(data_plot_revenue.index.values, data_plot_revenue[\"mean\"].values-data_plot_revenue[\"std\"].values, data_plot_revenue[\"mean\"].values+data_plot_revenue[\"std\"].values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e945965a",
   "metadata": {},
   "source": [
    "#### This one is better "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def1fe11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS THING HAS IDMEDIATLY THE 95% interval\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10,4), sharey=\"col\", sharex=True)     \n",
    "sns.lineplot(x=\"season\", y=\"pagerank\", hue=\"speaker\", data=df_rachel_ross, ax=axs[0],)\n",
    "sns.lineplot(x=\"season\", y=\"outdegree\", hue=\"speaker\",  data=df_rachel_ross, ax=axs[1],)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44956967",
   "metadata": {},
   "source": [
    "### Plot a graph with subgraphs: \n",
    "\n",
    "df_RFA_year = df.groupby([\"YEA\",\"TGT\"]).agg('count').reset_index().groupby(\"YEA\")[\"VOT\"].count().reset_index()\n",
    "df_proportion_positive = df.groupby([\"YEA\"]).VOT.apply(lambda x: np.mean(x > 0)).reset_index()\n",
    "df_votes_rfa_year = df.groupby([\"YEA\",\"TGT\"])[\"VOT\"].agg(\"count\").reset_index().groupby([\"YEA\"])[\"VOT\"].mean().reset_index()\n",
    "\n",
    "fig, ax = plt.subplots(1,3, figsize= (10,4),sharex=False,sharey=False)\n",
    "fig.tight_layout()\n",
    "df_RFA_year.plot(kind='line',x=\"YEA\",y=\"VOT\", ax=ax[0],ylabel=\"Number of RFA per year\",xlabel=\"Year\")\n",
    "df_proportion_positive.plot(ax=ax[1], kind='line', y=\"VOT\",x=\"YEA\",ylabel=\"Prportion of positive votes\",xlabel=\"Year\")\n",
    "df_votes_rfa_year.plot(ax=ax[2], kind='line', y=\"VOT\",x=\"YEA\",ylabel=\"Average number of votes per year\",xlabel=\"Year\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a581f74",
   "metadata": {},
   "source": [
    "## 3) Describing data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2578ec3",
   "metadata": {},
   "source": [
    "- `df['IncomePerCap'].describe()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7c6c43",
   "metadata": {},
   "source": [
    "### Statistical tests to see distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0370d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats import diagnostic\n",
    "diagnostic.kstest_normal(df['IncomePerCap'].values, dist = 'norm') # TEST IF THE DISTRO IS NORMAL\n",
    "diagnostic.kstest_normal(df['IncomePerCap'].values, dist = 'exp') #TEST IF THE DISTRO IS EXPONENTIAL "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfeedcdc",
   "metadata": {},
   "source": [
    "This returns the test statistic (first return) and p_val (second one). The null in both is \"data is from a normal/expo distro\". This is a goodness-of-fit test. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03638282",
   "metadata": {},
   "source": [
    "### Sample from data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8eb9c6a",
   "metadata": {},
   "source": [
    "- `sample1_counties = df.sample(n = 10, replace = True/False)`: sample n samples from a df with or without replacement. \n",
    "- `sample2_counties = df.sample(n = 10, replace = False, weights = df['TotalPop'])`: sample here with a biais. Countries with higher population will have higher chances to be sampled. To have opposite we can do for instance 1/totalpop. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b17281f",
   "metadata": {},
   "source": [
    "### Statistical tests to see relation between variables: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a8318f",
   "metadata": {},
   "source": [
    "- `stats.pearsonr(df['IncomePerCap'],df['Employed'])`: This ones calculates the pearson correlation between and returns the p_value between them as well. If +1: positive linear, 0: nothing LINEAR and -1: negative linear. It really tests if there is a linear relation. \n",
    "- `stats.spearmanr(df['IncomePerCap'],df['Employed'])`: This tests if there is a monotonic relation even if there is no linear relation. \"“As IncomePerCap increases, does Employed generally go up or down — even if not in a straight line?”. Usually higher than the other one. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77161391",
   "metadata": {},
   "source": [
    "### T-Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f013648",
   "metadata": {},
   "source": [
    "- `stats.ttest_ind(a, b)`: different individuals in different groups. Tests both sides if they are different (both-sided). Recommended to have: equal_var=False. \n",
    "- `stats.ttest_rel(before, after)`: same subject measured twice (before and after treatment for example). \n",
    "- `stats.ttest_1samp(sample, popmean=50000)`:  Compare a sample mean to a known value. \n",
    "- `stats.ttest_ind(a, b, alternative='greater/less')`:  specifiy the alternative. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19754fb1",
   "metadata": {},
   "source": [
    "#### Bootstrapping func:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7998585a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_confidence_interval(data, iterations=1000):\n",
    "    \"\"\"\n",
    "    Bootstrap the 95% confidence interval for the mean of the data.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: An array of data\n",
    "    - iterations: The number of bootstrap samples to generate\n",
    "    \n",
    "    Returns:\n",
    "    - A tuple representing the lower and upper bounds of the 95% confidence interval\n",
    "    \"\"\"\n",
    "    means = np.zeros(iterations)\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        bootstrap_sample = np.random.choice(data, size=len(data), replace=True)\n",
    "        means[i] = np.mean(bootstrap_sample)\n",
    "        \n",
    "    lower_bound = np.percentile(means, 2.5)\n",
    "    upper_bound = np.percentile(means, 97.5)\n",
    "    \n",
    "    return (lower_bound, upper_bound)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4ed188",
   "metadata": {},
   "source": [
    "## 4) Regression analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a22260f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf; import numpy as np; import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9bf100",
   "metadata": {},
   "source": [
    "#### Linear model for OLS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba4b1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = smf.ols(formula='time ~ C(diabetes) + C(high_blood_pressure)', data=df) #time = coef*diabetes + coef*hBP\n",
    "np.random.seed(2)\n",
    "res = mod.fit()\n",
    "print(res.summary()) #We can see Rsquare (fraction explained variance), coefs values, standard devs and Pvalues of the coefs. And CI\n",
    "\n",
    "res.predict() #OBTAIN RESULTS LIKE THE PROPENSITY SCORES !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b84a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = smf.ols(formula='time ~ C(high_blood_pressure) * C(DEATH_EVENT,  Treatment(reference=0)) + C(diabetes)',\n",
    "              data=df)\n",
    "#THE * term indiicates sum of each one and an interaction term !!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4798bc",
   "metadata": {},
   "source": [
    "#### FOR CONTONIOUS VARIABLES STANDARIZE USING MEANS AND STD !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be925e5",
   "metadata": {},
   "source": [
    "From the res variable: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf83fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = res.params.index ; coefficients = res.params.values ; p_values = res.pvalues ; standard_errors = res.bse.values \n",
    "res.conf_int() #confidence intervals\n",
    "#FANCY plotting to see results\n",
    "l1, l2, l3, l4 = zip(*sorted(zip(coefficients[1:], variables[1:], standard_errors[1:], p_values[1:]))) #We start from second coef (not intercept)\n",
    "plt.errorbar(l1, np.array(range(len(l1))), xerr= 2*np.array(l3), linewidth = 1,\n",
    "             linestyle = 'none',marker = 'o',markersize= 3,\n",
    "             markerfacecolor = 'black',markeredgecolor = 'black', capsize= 5)\n",
    "plt.vlines(0,0, len(l1), linestyle = '--')\n",
    "plt.yticks(range(len(l2)),l2);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9a7576",
   "metadata": {},
   "source": [
    "WHEN STANDARD-> AN INCREASE IN 1 IN PREDICTOR -> INCREASE IN +(COEF) OF LOG ODDS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5634445",
   "metadata": {},
   "source": [
    "#### LOGISTIC REGRESSION (YES OR NO OUTCOME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2eaaee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = smf.logit(formula='DEATH_EVENT ~ serum_creatinine', data=df) \n",
    "np.random.seed(2)\n",
    "res = mod.fit()\n",
    "print(res.summary()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c97f64",
   "metadata": {},
   "source": [
    "## 5) Causal analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705ec576",
   "metadata": {},
   "outputs": [],
   "source": [
    "### MAKE BAR PLOT WITH DIFFERENT CATEGORIES:  \n",
    "\n",
    "# race\n",
    "\n",
    "df['white'] = (~(df['black'].astype(bool) \\\n",
    "                    | df['hispan'].astype(bool))).astype(bool)\n",
    "\n",
    "lalonde_data_group = df.groupby(df.treat)[['white', 'black', 'hispan']].sum()\n",
    "lalonde_data_group = lalonde_data_group.div(lalonde_data_group.sum(axis=1), axis=0)\n",
    "pl = lalonde_data_group.plot(kind='bar', figsize=[8,4], rot=0)\n",
    "pl.set_title('race')\n",
    "pl.set_ylabel('participants')\n",
    "pl.set_xlabel('group')\n",
    "plt.show()\n",
    "\n",
    "# white outnumber the other races in the control group, and on the \n",
    "# other hand, in the treated group the proportion of black is almost \n",
    "# the only one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2f5be7",
   "metadata": {},
   "source": [
    "### PROPENSITY MATCHING ALGO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16580fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "treated_people = df.loc[df[\"treat\"]==1]\n",
    "control_people = df.loc[df[\"treat\"]==0]\n",
    "\n",
    "G = nx.Graph()\n",
    "\n",
    "def calculate_similarity(prop1,prop2):\n",
    "    return 1-np.abs(prop1-prop2)\n",
    "\n",
    "for control_idx, control_row in control_people.iterrows():\n",
    "    for treat_idx, treat_row in treated_people.iterrows(): \n",
    "        \n",
    "        if (treat_row[\"black\"] == control_row[\"black\"]) and (treat_row[\"hispan\"] == control_row[\"hispan\"]):\n",
    "            similarity = calculate_similarity(treat_row[\"propensity\"],control_row[\"propensity\"])\n",
    "            G.add_weighted_edges_from([(treat_idx,control_idx, similarity)])\n",
    "        \n",
    "matching = nx.max_weight_matching(G) \n",
    "matched = [item for t in matching for item in t]\n",
    "df_matched_try_2 = df.iloc[matched]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3869b17",
   "metadata": {},
   "source": [
    "## 6) Supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45c5331",
   "metadata": {},
   "source": [
    "### Liner regression or RIDGE LR (REGULARIZATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1181661e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, auc, roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c9af94",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = ['TV', 'radio', 'newspaper']\n",
    "X = data[feature_cols]\n",
    "y = data.sales\n",
    "\n",
    "lin_reg = LinearRegression()  # create the model\n",
    "lin_reg.fit(X, y)  # train it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e15b180",
   "metadata": {},
   "source": [
    "#### RIDGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30039381",
   "metadata": {},
   "outputs": [],
   "source": [
    "#JUST REPLACE THE LIN_reg by ridge and that's it:\n",
    "feature_cols = ['TV', 'radio', 'newspaper']\n",
    "X = data[feature_cols]\n",
    "y = data.sales\n",
    "\n",
    "model = Ridge(alpha=5)  # create the model\n",
    "model.fit(X, y)  # train it #IT WILL HAVE SAME ATTRIBUTES AS THE OTHER ONE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df8168b",
   "metadata": {},
   "source": [
    "The `LinearRegression()` class has attributes `coef_` and `intercept_` that represents the values of the coeficcients and the intercept of the linear regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2319154b",
   "metadata": {},
   "source": [
    "#### Plot linear regression graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2eb4860",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "\n",
    "# cross_val_predict returns an array of the same size as `y` where each entry\n",
    "# is a prediction obtained by cross validation:\n",
    "predicted = cross_val_predict(lr, X, y, cv=5)\n",
    "\n",
    "# Plot the results\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax.scatter(y, predicted, edgecolors=(0, 0, 0))\n",
    "ax.plot([min(y), max(y)], [min(y), max(y)], 'r--', lw=4)\n",
    "ax.set_xlabel('Original')\n",
    "ax.set_ylabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3096db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y, predicted) #PRINT MEAN SQUARE ERROR OF THE ESTIMATOR "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2224f74b",
   "metadata": {},
   "source": [
    "#### Example basics with Logistic reg\n",
    "\n",
    "- `X = pd.get_dummies(titanic[titanic_features]) and X = X.fillna(X.mean())`: encode numerical features for regression and fill nan\n",
    "- `logistic = LogisticRegression(solver='lbfgs')`. \n",
    "- `precision = cross_val_score(logistic, X, y, cv=10, scoring=\"precision\")`: can return mean and standard dev\n",
    "- `recall = cross_val_score(logistic, X, y, cv=10, scoring=\"recall\")`: can return mean and std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637ed08a",
   "metadata": {},
   "source": [
    "#### Example ROC curve: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf8a503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the probabilities with a cross validationn\n",
    "y_pred = cross_val_predict(logistic, X, y, cv=10, method=\"predict_proba\")\n",
    "# Compute the False Positive Rate and True Positive Rate\n",
    "fpr, tpr, _ = roc_curve(y, y_pred[:, 1])\n",
    "# Compute the area under the fpt-tpf curve\n",
    "auc_score = auc(fpr, tpr)\n",
    "plt.plot(fpr, tpr)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve - Area = {:.5f}\".format(auc_score));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c6369c",
   "metadata": {},
   "source": [
    "#### Predict: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8311b34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = LogisticRegression(solver='lbfgs')\n",
    "logistic.fit(X.values, y)\n",
    "logistic.predict([[25,0,0,100,False,True]]) #RETURN VALUE\n",
    "logistic.predict_proba([[25,0,0,100,False,True]]) #RETURN probabilty distribution #####DONT FORGET THE DOUBLE []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8d38e3",
   "metadata": {},
   "source": [
    "#### Random forest:\n",
    "\n",
    "- `forest = RandomForestClassifier(n_estimators=number,max_depth=depth, random_state=0)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac654698",
   "metadata": {},
   "source": [
    "## 7) Applied ML "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a5abdf",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bb6678",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_set(data_to_split, ratio=0.8):\n",
    "    \"\"\"Returns train and test data using a division\"\"\"\n",
    "    mask = np.random.rand(len(data_to_split)) < ratio\n",
    "    return [data_to_split[mask].reset_index(drop=True), data_to_split[~mask].reset_index(drop=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc353235",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_std = pd.DataFrame()\n",
    "for c in train_features.columns:\n",
    "    train_features_std[c] = (train_features[c]-means[c])/stddevs[c]\n",
    "    \n",
    "### STANDARIZE IN ONE GO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd326e8c",
   "metadata": {},
   "source": [
    "### Confusion matrix + metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ab5262",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_confusion_matrix(true_label, prediction_proba, decision_threshold=0.5): \n",
    "    \n",
    "    predict_label = (prediction_proba[:,1]>decision_threshold).astype(int)   \n",
    "                                                                                                                       \n",
    "    TP = np.sum(np.logical_and(predict_label==1, true_label==1))\n",
    "    TN = np.sum(np.logical_and(predict_label==0, true_label==0))\n",
    "    FP = np.sum(np.logical_and(predict_label==1, true_label==0))\n",
    "    FN = np.sum(np.logical_and(predict_label==0, true_label==1))\n",
    "    \n",
    "    confusion_matrix = np.asarray([[TP, FP],\n",
    "                                    [FN, TN]])\n",
    "    return confusion_matrix\n",
    "\n",
    "def confusion_plot(actual, predicted):\n",
    "    # Standard confusion matrix: [[TN, FP], [FN, TP]]\n",
    "    cm = sklearn.metrics.confusion_matrix(actual, predicted)\n",
    "    TN, FP, FN, TP = cm.ravel()\n",
    "    # Reorder so TP is top-left\n",
    "    cm_reordered = np.array([[TP, FN],[FP, TN]])\n",
    "    # Labels inside cells\n",
    "    labels = np.array([[f\"TP\\n{TP}\", f\"FN\\n{FN}\"],[f\"FP\\n{FP}\", f\"TN\\n{TN}\"]])\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm_reordered,annot=labels,fmt=\"\",cbar=False,linewidths=1,linecolor=\"black\")\n",
    "    plt.title(\"Confusion Matrix\", fontsize=16, pad=20)\n",
    "    plt.xlabel(\"Prediction\", fontsize=13)\n",
    "    plt.ylabel(\"Real\", fontsize=13)\n",
    "    # Axis tick labels\n",
    "    plt.xticks([0.5, 1.5], [\"Positive\", \"Negative\"])\n",
    "    plt.yticks([0.5, 1.5], [\"Positive\", \"Negative\"], rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def compute_all_score(confusion_matrix, t=0.5):\n",
    "    [[TP, FP],[FN, TN]] = confusion_matrix.astype(float)\n",
    "    \n",
    "    accuracy =  (TP+TN)/np.sum(confusion_matrix)\n",
    "    \n",
    "    precision_positive = TP/(TP+FP) if (TP+FP) !=0 else np.nan\n",
    "    precision_negative = TN/(TN+FN) if (TN+FN) !=0 else np.nan\n",
    "    \n",
    "    recall_positive = TP/(TP+FN) if (TP+FN) !=0 else np.nan\n",
    "    recall_negative = TN/(TN+FP) if (TN+FP) !=0 else np.nan\n",
    "\n",
    "    F1_score_positive = 2 *(precision_positive*recall_positive)/(precision_positive+recall_positive) if (precision_positive+recall_positive) !=0 else np.nan\n",
    "    F1_score_negative = 2 *(precision_negative*recall_negative)/(precision_negative+recall_negative) if (precision_negative+recall_negative) !=0 else np.nan\n",
    "\n",
    "    return [t, accuracy, precision_positive, recall_positive, F1_score_positive, precision_negative, recall_negative, F1_score_negative]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b53ac9",
   "metadata": {},
   "source": [
    "### Plot coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1c0991",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(solver=\"lbfgs\",max_iter=10000)\n",
    "model.fit(X_train,Y_train.Adoption)\n",
    "coeff_df = pd.DataFrame({\"Feature\": X_train.columns.values, \"Coefficient\": model.coef_[0]})\n",
    "coef_df_sorted = coeff_df.sort_values(by=\"Coefficient\", ascending=True)\n",
    "\n",
    "# Create plot.\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.barh(coef_df_sorted[\"Feature\"], coef_df_sorted[\"Coefficient\"], color=\"blue\")\n",
    "plt.xlabel(\"Coefficient Value\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(\"Feature Importance (Linear Regression Coefficients)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b1f639",
   "metadata": {},
   "source": [
    "## 8) Unsupervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d25d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaled_features = StandardScaler().fit(df).transform(df) #THIS BASICALLY NORMALIZES THINGS UP !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a1ebb0",
   "metadata": {},
   "source": [
    "#### Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d7a4f8",
   "metadata": {},
   "source": [
    "- `kmean = KMeans(n_clusters=k, random_state=0).fit(X)`.\n",
    "- `kmean.labels_`: Get the classification results (0,1, 2, ...) for each data point. \n",
    "- `kmean.cluster_centers_`:  coordiniates of centers. \n",
    "- `labels = KMeans(n_clusters=k,random_state=0).fit_predict(X)`: Directly get the data labels. \n",
    "- `silhouette_score(X, labels)`: get silhoutte scores. \n",
    "- `kmean.inertia_`: Get the sum of square errors for the specific k value. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3d5ecc",
   "metadata": {},
   "source": [
    "#### Silhoutte score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df86b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouettes = []\n",
    "\n",
    "# Try multiple k\n",
    "for k in range(2, 11):\n",
    "    # Cluster the data and assigne the labels\n",
    "    labels = KMeans(n_clusters=k, random_state=10).fit_predict(X)\n",
    "    # Get the Silhouette score\n",
    "    score = silhouette_score(X, labels)\n",
    "    silhouettes.append({\"k\": k, \"score\": score})\n",
    "    \n",
    "# Convert to dataframe\n",
    "silhouettes = pd.DataFrame(silhouettes)\n",
    "\n",
    "# Plot the data\n",
    "plt.plot(silhouettes.k, silhouettes.score)\n",
    "plt.xlabel(\"K\")\n",
    "plt.ylabel(\"Silhouette score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693ac95d",
   "metadata": {},
   "source": [
    "#### Elbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ec0fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sse(features_X, start=2, end=11):\n",
    "    sse = []\n",
    "    for k in range(start, end):\n",
    "        # Assign the labels to the clusters\n",
    "        kmeans = KMeans(n_clusters=k, random_state=10).fit(features_X)\n",
    "        sse.append({\"k\": k, \"sse\": kmeans.inertia_})\n",
    "\n",
    "    sse = pd.DataFrame(sse)\n",
    "    # Plot the data\n",
    "    plt.plot(sse.k, sse.sse)\n",
    "    plt.xlabel(\"K\")\n",
    "    plt.ylabel(\"Sum of Squared Errors\")\n",
    "    \n",
    "plot_sse(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5e4a29",
   "metadata": {},
   "source": [
    "#### Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbb409b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduced_tsne = TSNE(n_components=2, init='random', learning_rate='auto', random_state=0).fit_transform(X10d)\n",
    "X_reduced_pca = PCA(n_components=2).fit(X10d).transform(X10d)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(7,3), sharey=True)\n",
    "\n",
    "# Cluster the data in 3 groups\n",
    "labels = KMeans(n_clusters=3, random_state=0).fit_predict(X10d)\n",
    "\n",
    "# Plot the data reduced in 2d space with t-SNE\n",
    "axs[0].scatter(X_reduced_tsne[:,0], X_reduced_tsne[:,1], c=labels, alpha=0.6)\n",
    "axs[0].set_title(\"t-SNE\")\n",
    "\n",
    "# Plot the data reduced in 2d space with PCA\n",
    "axs[1].scatter(X_reduced_pca[:,0], X_reduced_pca[:,1], c=labels, alpha=0.6)\n",
    "axs[1].set_title(\"PCA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34690524",
   "metadata": {},
   "source": [
    "## 9) Handling text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52e3301",
   "metadata": {},
   "source": [
    "#### Load lists of books "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef758c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "books = list()\n",
    "\n",
    "for book_file in os.listdir(corpus_root):\n",
    "    if \".txt\" in book_file:\n",
    "        print(book_file)\n",
    "        with codecs.open(os.path.join(corpus_root,book_file),encoding=\"utf8\") as f:\n",
    "            books.append(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70fc3f4",
   "metadata": {},
   "source": [
    "#### Remove new lines:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9028300",
   "metadata": {},
   "source": [
    "books = [\" \".join(b.split()) for b in books]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb1e9da",
   "metadata": {},
   "source": [
    "#### Split a line into two:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a978db35",
   "metadata": {},
   "outputs": [],
   "source": [
    "character, line = (string).split(\": \", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433aca69",
   "metadata": {},
   "source": [
    "#### Clean lines in a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4195677e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_line(line):\n",
    "    for char in EXCLUDE_CHARS:\n",
    "        line = line.replace(char, ' ')\n",
    "    return line.lower()\n",
    "lines[\"Line\"] = lines[\"Line\"].apply(clean_line)\n",
    "lines.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48c286d",
   "metadata": {},
   "source": [
    "#### Corpus frequency: calculate and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fc77d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_frequency = pd.concat([pd.Series(row['Line'].split(' ')) for _, row in lines.iterrows()]).reset_index()\n",
    "corpus_frequency.columns = [\"Frequency\", \"Word\"] #Calculate frequency of each word\n",
    "corpus_frequency = corpus_frequency.groupby(\"Word\").count() #have each word with its frequency\n",
    "\n",
    "corpus_frequency.plot.hist(column=[\"Frequency\"], bins=100, title=\"Frequency histogram\")\n",
    "corpus_frequency.plot.hist(column=[\"Frequency\"], loglog=True, bins=np.logspace(0, 6, 100),\n",
    "                           title=\"Frequency histogram (loglog scale)\");\n",
    "#Plot normal histogram and loglog scale version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49806e20",
   "metadata": {},
   "source": [
    "#### How many words characters say"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4611ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines[\"Words\"] = lines[\"Line\"].apply(lambda x: len(x.split()))\n",
    "words_per_char = lines.groupby(\"Character\").sum()[\"Words\"]\n",
    "words_per_char[recurrent_chars.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d6ed5b",
   "metadata": {},
   "source": [
    "### Use Tf-IDF matrix: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e66d3e",
   "metadata": {},
   "source": [
    "#### Build Tf-IDF matrix: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4e41c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\", min_df=3, max_df=0.9)\n",
    "X = vectorizer.fit_transform(df[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c338a22d",
   "metadata": {},
   "source": [
    "#### Manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68482a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 236 #Number of episodes or documents\n",
    "n = len(L)\n",
    "X = np.zeros((m,n))\n",
    "\n",
    "for i,episode in enumerate(df_tokens_per_ep[\"episode\"].values): \n",
    "    for j,token in enumerate(List_of_all_tokens): \n",
    "        tokens_in_ep = df_tokens_per_ep.loc[df_tokens_per_ep[\"episode\"] == episode][\"token_flat\"].values.tolist()[0]\n",
    "        X[i,j] = tokens_in_ep.count(token)\n",
    "\n",
    "Tf = X\n",
    "IDF = np.log(m/np.count_nonzero(X, axis=0))\n",
    "tf_idf = Tf*IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32456dd",
   "metadata": {},
   "source": [
    "#### Convert to pandas: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4526424e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df = pd.DataFrame(X.toarray(),columns=vectorizer.get_feature_names_out(),index=df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71461276",
   "metadata": {},
   "source": [
    "#### Most used words by someone: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f598e751",
   "metadata": {},
   "outputs": [],
   "source": [
    "alice_docs = tfidf_df.loc[df[\"author\"] == \"alice\"]\n",
    "alice_mean = alice_docs.mean()\n",
    "alice_top_words = alice_mean.sort_values(ascending=False) #.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0026132",
   "metadata": {},
   "source": [
    "#### Most used words for a specific outcome: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81a108d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create docs \n",
    "funny_docs = tfidf_df.loc[df[\"label\"] == \"funny\"]\n",
    "not_funny_docs = tfidf_df.loc[df[\"label\"] == \"not_funny\"]\n",
    "\n",
    "#Compute means \n",
    "funny_mean = funny_docs.mean()\n",
    "not_funny_mean = not_funny_docs.mean()\n",
    "\n",
    "#Contrast them\n",
    "diff = funny_mean - not_funny_mean\n",
    "\n",
    "#PRint results and visualize\n",
    "diff.sort_values(ascending=False).head(20)   # words specific to funny\n",
    "diff.sort_values().head(20)                  # words specific to not funny\n",
    "\n",
    "#Visualize \n",
    "diff.sort_values(ascending=False).head(15).plot.barh()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a3f1d8",
   "metadata": {},
   "source": [
    "### Train model to identify source of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35aa3552",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=[\"text\", \"author\"]).copy()\n",
    "df[\"text\"] = df[\"text\"].astype(str)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"text\"],df[\"author\"],test_size=0.2,random_state=42,stratify=df[\"author\"])\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#Option A\n",
    "clf = Pipeline([(\"tfidf\", TfidfVectorizer(ngram_range=(1,2),     min_df=3,max_df=0.9,stop_words=\"english\")),\n",
    "                (\"model\", LogisticRegression(max_iter=2000,class_weight=\"balanced\"))])\n",
    "\n",
    "#Option B\n",
    "from sklearn.svm import LinearSVC\n",
    "clf = Pipeline([(\"tfidf\", TfidfVectorizer(ngram_range=(1,2), min_df=3, max_df=0.9)),(\"model\", LinearSVC())])\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "#Evaluate \n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "pred = clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, pred))\n",
    "print(classification_report(y_test, pred))\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "cm = confusion_matrix(y_test, pred, labels=clf.classes_)\n",
    "cm_df = pd.DataFrame(cm, index=clf.classes_, columns=clf.classes_)\n",
    "\n",
    "#PRedict:\n",
    "new_text = \"bro this lab report is killing me\"\n",
    "print(clf.predict([new_text])[0])\n",
    "\n",
    "#Probabilities\n",
    "proba = clf.predict_proba([new_text])[0]\n",
    "top = sorted(zip(clf.classes_, proba), key=lambda x: x[1], reverse=True)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52227b9",
   "metadata": {},
   "source": [
    "#### Determine character by use of unique words: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea134a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_for_chars = pd.concat([pd.Series(row[\"Character\"], row['Line'].split())\n",
    "                             for _, row in train_set.iterrows()]).reset_index()\n",
    "words_for_chars.columns = [\"Word\", \"Character\"] #Here we have a dataset in which each word has besides who said it\n",
    "\n",
    "words_for_chars = words_for_chars.groupby(\"Word\")[\"Character\"].apply(set) #here we have each word and all people who said it\n",
    "sheldon_words = words_for_chars[words_for_chars.apply(lambda x: (\"Sheldon\" in x) and (len(x) == 1))].index\n",
    "\n",
    "#Verify if one column has at least some word\n",
    "def contains_sheldon_words(line):\n",
    "    for word in sheldon_words:\n",
    "        if word in line.split():\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "#Classify a line if it has a certain given word\n",
    "test_pred = test_set[\"Line\"].apply(contains_sheldon_words)\n",
    "test_true = test_set[\"Character\"] == \"Sheldon\"\n",
    "\n",
    "print(\"Accuracy: \", (test_true == test_pred).sum() / len(test_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d408376",
   "metadata": {},
   "source": [
    "### Create a boolean in case a word is present in one column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709fe1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Quaker'] = ['quaker' in role.lower() for role in df.Role]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd098aa",
   "metadata": {},
   "source": [
    "## 10) Handling Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab12f9ab",
   "metadata": {},
   "source": [
    "### Helpers for degree distribution, description and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b06602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for plotting the degree distribution of a Graph\n",
    "def plot_degree_distribution(G):\n",
    "    degrees = {}\n",
    "    for node in G.nodes():\n",
    "        degree = G.degree(node)\n",
    "        if degree not in degrees:\n",
    "            degrees[degree] = 0\n",
    "        degrees[degree] += 1\n",
    "    sorted_degree = sorted(degrees.items())\n",
    "    deg = [k for (k,v) in sorted_degree]\n",
    "    cnt = [v for (k,v) in sorted_degree]\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.bar(deg, cnt, width=0.80, color='b')\n",
    "    plt.title(\"Degree Distribution\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.xlabel(\"Degree\")\n",
    "    ax.set_xticks([d+0.05 for d in deg])\n",
    "    ax.set_xticklabels(deg)\n",
    "    \n",
    "# Helper function for printing various graph properties\n",
    "def describe_graph(G):\n",
    "    print(G)\n",
    "    if nx.is_connected(G):\n",
    "        print(\"Avg. Shortest Path Length: %.4f\" %nx.average_shortest_path_length(G))\n",
    "        print(\"Diameter: %.4f\" %nx.diameter(G)) # Longest shortest path\n",
    "    else:\n",
    "        print(\"Graph is not connected\")\n",
    "        print(\"Diameter and Avg shortest path length are not defined!\")\n",
    "    print(\"Sparsity: %.4f\" %nx.density(G))  # #edges/#edges-complete-graph\n",
    "    # #closed-triplets(3*#triangles)/#all-triplets\n",
    "    print(\"Global clustering coefficient aka Transitivity: %.4f\" %nx.transitivity(G))\n",
    "    \n",
    "# Helper function for visualizing the graph\n",
    "def visualize_graph(G, with_labels=True, k=None, alpha=1.0, node_shape='o'):\n",
    "    #nx.draw_spring(G, with_labels=with_labels, alpha = alpha)\n",
    "    pos = nx.spring_layout(G, k=k) #k is how much points are together\n",
    "    if with_labels:\n",
    "        lab = nx.draw_networkx_labels(G, pos, labels=dict([(n, n) for n in G.nodes()]))\n",
    "    ec = nx.draw_networkx_edges(G, pos, alpha=alpha) #Alpha is transparency\n",
    "    nc = nx.draw_networkx_nodes(G, pos, nodelist=G.nodes(), node_color='g', node_shape=node_shape)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd33b0d",
   "metadata": {},
   "source": [
    "#### PLot simple:\n",
    "\n",
    "nx.draw_spring(G, with_labels=True,  alpha = 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c14523",
   "metadata": {},
   "source": [
    "#### Plot smaller, tighter and with connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a1ed6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_graph(quakerG, False, k=0.2, alpha=0.4, node_shape='.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e37fe30",
   "metadata": {},
   "source": [
    "### Creating a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dbbc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph() # for a directed graph use nx.DiGraph()\n",
    "G.add_node(1); G.add_nodes_from(range(2,9)) #-> to add multiple nodes at once\n",
    "# add edges \n",
    "G.add_edge(1,2); edges = [(2,3), (1,3), (4,1), (4,5), (5,6), (5,7), (6,7), (7,8), (6,8)]\n",
    "G.add_edges_from(edges); G.nodes() #-> print nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fef8b2c",
   "metadata": {},
   "source": [
    "### Graph from an edgelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e582fa91",
   "metadata": {},
   "outputs": [],
   "source": [
    "quakerG = nx.from_pandas_edgelist(edges, 'Source', 'Target', edge_attr=None, create_using= nx.Graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5fe9fe",
   "metadata": {},
   "source": [
    "#### Add attributes to a graph and print them of a node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557db953",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.set_node_attributes(quakerG, df['Gender'].to_dict(), 'Gender' ) #Add attribute \n",
    "quakerG.nodes['William Penn'] #Print attributes of a node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c08d8ae",
   "metadata": {},
   "source": [
    "#### Sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7196bd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Network sparsity: %.4f\" %nx.density(quakerG))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e5224c",
   "metadata": {},
   "source": [
    "#### Connected components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2097197",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nx.is_connected(quakerG))\n",
    "comp = list(nx.connected_components(quakerG))\n",
    "print('The graph contains', len(comp), 'connected components')\n",
    "\n",
    "####Largest component \n",
    "largest_comp = max(comp, key=len)\n",
    "percentage_lcc = len(largest_comp)/quakerG.number_of_nodes() * 100\n",
    "print('The largest component has', len(largest_comp), 'nodes', 'accounting for %.2f'% percentage_lcc, '% of the nodes') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c3473f",
   "metadata": {},
   "source": [
    "#### Diameter and shortest path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08d37f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fell_whitehead_path = nx.shortest_path(quakerG, source=\"Margaret Fell\", target=\"George Whitehead\")\n",
    "print(\"Shortest path between Fell and Whitehead:\", fell_whitehead_path)\n",
    "\n",
    "# take the largest component and analyse its diameter = longest shortest-path\n",
    "lcc_quakerG = quakerG.subgraph(largest_comp)\n",
    "print(\"The diameter of the largest connected component is\", nx.diameter(lcc_quakerG))\n",
    "print(\"The avg shortest path length of the largest connected component is\", nx.average_shortest_path_length(lcc_quakerG))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5932ad3",
   "metadata": {},
   "source": [
    "### Transititivy and clustering coefs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383be945",
   "metadata": {},
   "source": [
    "#### Transitivity of general network: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391f411a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('%.4f' %nx.transitivity(quakerG))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4524439",
   "metadata": {},
   "source": [
    "#### Clustering coefs of a given node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354338d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nx.clustering(quakerG, ['Alexander Parker', 'John Crook']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3fd658",
   "metadata": {},
   "source": [
    "### Subgraphs: create and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52b0720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subgraphs and plot \n",
    "subgraph_Alex = quakerG.subgraph(['Alexander Parker']+list(quakerG.neighbors('Alexander Parker')))\n",
    "subgraph_John = quakerG.subgraph(['John Crook']+list(quakerG.neighbors('John Crook')))\n",
    "\n",
    "nx.draw_spring(subgraph_Alex, with_labels=True)\n",
    "nx.draw_circular(subgraph_John, with_labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3029bf8",
   "metadata": {},
   "source": [
    "### Importance: degrees, and centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d176c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter; import collections\n",
    "\n",
    "degrees = dict(quakerG.degree(quakerG.nodes()))\n",
    "sorted_degree = sorted(degrees.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "# And the top 5 most popular quakers are.. \n",
    "for quaker, degree in sorted_degree[:5]:\n",
    "    print(quaker, 'who is', quakerG.nodes[quaker]['Role'], 'knows', degree, 'people')\n",
    " \n",
    "#Print degree distribution    \n",
    "degree_seq = [d[1] for d in sorted_degree]\n",
    "degreeCount = collections.Counter(degree_seq)\n",
    "degreeCount = pd.DataFrame.from_dict( degreeCount, orient='index').reset_index()\n",
    "fig = plt.figure()\n",
    "ax = plt.gca()\n",
    "ax.plot(degreeCount['index'], degreeCount[0], 'o', c='blue', markersize= 4)\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('Degree')\n",
    "plt.title('Degree distribution for the Quaker network')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8073c773",
   "metadata": {},
   "source": [
    "#### Kantz centrality (generalization of degree) and Betweeness centrality (shortest paths that come through u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8bc49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kantz centrality\n",
    "degrees = dict(quakerG.degree(quakerG.nodes()))\n",
    "katz = nx.katz_centrality(quakerG)\n",
    "nx.set_node_attributes(quakerG, katz, 'katz')\n",
    "sorted_katz = sorted(katz.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "# And the top 5 most popular quakers are.. \n",
    "for quaker, katzc in sorted_katz[:5]:\n",
    "    print(quaker, 'who is', quakerG.nodes[quaker]['Role'], 'has katz-centrality: %.3f' %katzc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992f53af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute betweenness centrality\n",
    "betweenness = nx.betweenness_centrality(quakerG)\n",
    "# Assign the computed centrality values as a node-attribute in your network\n",
    "nx.set_node_attributes(quakerG, betweenness, 'betweenness')\n",
    "sorted_betweenness = sorted(betweenness.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "for quaker, bw in sorted_betweenness[:5]:\n",
    "    print(quaker, 'who is', quakerG.nodes[quaker]['Role'], 'has betweeness: %.3f' %bw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbe6f10",
   "metadata": {},
   "source": [
    "### plot betweeness centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcb7551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# similar pattern\n",
    "list_nodes =list(quakerG.nodes())\n",
    "list_nodes.reverse()   # for showing the nodes with high betweeness centrality \n",
    "pos = nx.spring_layout(quakerG)\n",
    "ec = nx.draw_networkx_edges(quakerG, pos, alpha=0.1)\n",
    "nc = nx.draw_networkx_nodes(quakerG, pos, nodelist=list_nodes, node_color=[quakerG.nodes[n][\"betweenness\"] for n in list_nodes], \n",
    "                            alpha=0.8, node_shape = '.')\n",
    "plt.colorbar(nc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dde56e",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4374c75",
   "metadata": {},
   "source": [
    "#### Girvan newman: edges with high betweeness centrality separate communities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6aaf388",
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx.algorithms.community.centrality import girvan_newman\n",
    "import itertools\n",
    "comp = girvan_newman(G)\n",
    "it = 0\n",
    "for communities in itertools.islice(comp, 4):\n",
    "    it +=1\n",
    "    print('Iteration', it)\n",
    "    print(tuple(sorted(c) for c in communities)) \n",
    "    \n",
    "    \n",
    "# choose which \"level\" to plot:\n",
    "level = 3\n",
    "communities = next(itertools.islice(comp, level-1, None))  # tuple of sets\n",
    "\n",
    "gn_partition = {}\n",
    "for cid, comm in enumerate(communities):\n",
    "    for node in comm:\n",
    "        gn_partition[node] = cid\n",
    "\n",
    "nx.set_node_attributes(G, gn_partition, \"girvan\")\n",
    "pos = nx.spring_layout(G, k=0.2, seed=42)\n",
    "nx.draw_networkx_edges(G, pos, alpha=0.2)\n",
    "nx.draw_networkx_nodes(G, pos,nodelist=list(G.nodes()),node_color=[gn_partition[n] for n in G.nodes()],node_size=100,cmap=plt.cm.jet)\n",
    "plt.axis(\"off\")\n",
    "plt.title(f\"Girvan–Newman communities (level={level}, k={len(communities)})\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac8dbba",
   "metadata": {},
   "source": [
    "#### Louvain: every node is initially a community and we then just start binding and binding to see if we can have better clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68dcfcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from community import community_louvain\n",
    "\n",
    "partition = community_louvain.best_partition(quakerG)\n",
    "# add it as an attribute to the nodes\n",
    "for n in quakerG.nodes:\n",
    "    quakerG.nodes[n][\"louvain\"] = partition[n]\n",
    "\n",
    "#Plot: \n",
    "pos = nx.spring_layout(quakerG,k=0.2)\n",
    "ec = nx.draw_networkx_edges(quakerG, pos, alpha=0.2)\n",
    "nc = nx.draw_networkx_nodes(quakerG, pos, nodelist=quakerG.nodes(), node_color=[quakerG.nodes[n][\"louvain\"] for n in quakerG.nodes], \n",
    "                            node_size=100, cmap=plt.cm.jet)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1d54fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recover members of a cluster\n",
    "cluster_James = partition['James Nayler']\n",
    "# Take all the nodes that belong to James' cluster\n",
    "members_c = [q for q in quakerG.nodes if partition[q] == cluster_James]\n",
    "# get info about these quakers\n",
    "for quaker in members_c:\n",
    "    print(quaker, 'who is', quakerG.nodes[quaker]['Role'], 'and died in ',quakerG.nodes[quaker]['Deathdate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef1ee0a",
   "metadata": {},
   "source": [
    "#### Homophily: or equivalent of correlation: ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ead4a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for numerical attributes, values must be integers\n",
    "nx.numeric_assortativity_coefficient(quakerG, 'Deathdate') #IF high -> means people who have died at same times were likely linked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4098e9b1",
   "metadata": {},
   "source": [
    "### Get edges respecting a certain characteristic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7427e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_episode = nx.get_edge_attributes(G, \"episode\")\n",
    "for episode in list_of_episodes: \n",
    "    edges_current_episode = [k for k,v in edge_episode.items() if v==episode]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9225a0be",
   "metadata": {},
   "source": [
    "#### Create subgraphs from attributes and store centrality measurements in df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65c9820",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_to_create_df = []\n",
    "list_of_characters = set(list(df_to_draw[\"speaker_x\"])+list(df_to_draw[\"speaker_y\"]))\n",
    "list_of_episodes = set(list(df_to_draw[\"episode\"]))\n",
    "edge_episode = nx.get_edge_attributes(G, \"episode\")\n",
    "main_characters = [\"Rachel Green\", \"Ross Geller\", \"Chandler Bing\", \"Monica Geller\", \"Joey Tribbiani\", \"Phoebe Buffay\"]\n",
    "for episode in list_of_episodes: \n",
    "    edges_current_episode = [k for k,v in edge_episode.items() if v==episode]\n",
    "    subgraph_episode = G.edge_subgraph(edges_current_episode)\n",
    "    page_rank_centrality = list(nx.pagerank(subgraph_episode).items())\n",
    "    out_degrees = {n: d for n, d in subgraph_episode.out_degree()}\n",
    "    characters_not_in_ep = list_of_characters - set(list(nx.pagerank(subgraph_episode).keys()))\n",
    "    \n",
    "    for character_in_episode,page_centre in page_rank_centrality: \n",
    "        list_to_create_df.append({\"speaker\": character_in_episode, \"pagerank\": page_centre, \"outdegree\": out_degrees[character_in_episode], \"episode\": episode})\n",
    "    for charecter_not_in_episode in characters_not_in_ep: \n",
    "        list_to_create_df.append({\"speaker\": characters_not_in_ep, \"pagerank\": 0, \"outdegree\": 0, \"episode\": episode})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b85d9f",
   "metadata": {},
   "source": [
    "#### ADD edges with a specific attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69abde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "attrs_to_add = dict(df_2004.set_index([\"SRC\", \"TGT\"]).VOT_RND) #HERE WE HAVE THE SOURCE AND TARGET NODE AND THE ATTRIBUTE TO ADD \n",
    "nx.set_edge_attributes(G, name=\"VOT_RND\", values=attrs_to_add)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FinalADA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
